LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name    | Type                  | Params
--------------------------------------------------
0 | model   | TinyNetwork           | 802 K
1 | loss_fn | CrossEntropyCriterion | 0
--------------------------------------------------
802 K     Trainable params
0         Non-trainable params
802 K     Total params
3.210     Total estimated model params size (MB)
/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/haitt/workspaces/codes/nas/zebanas/train.py", line 32, in main
    trainer.fit(model, datamodule)
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1033, in _run_stage
    self._run_sanity_check()
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py", line 1062, in _run_sanity_check
    val_loop.run()
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 134, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 391, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py", line 403, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/workspaces/codes/nas/zebanas/zebanas/tasks/classification.py", line 42, in validation_step
    loss = self.loss_fn(y_hat, y)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/workspaces/codes/nas/zebanas/zebanas/criterions/ce.py", line 14, in forward
    return self.ce(logits, targets.float())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1174, in forward
    return F.cross_entropy(input, target, weight=self.weight,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/haitt/mambaforge/envs/nas/lib/python3.11/site-packages/torch/nn/functional.py", line 3029, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Sanity Checking DataLoader 0:   0%|                                                                                                                              | 0/2 [00:00<?, ?it/s](tensor([[0.9277, 0.3999, 0.8642,  ..., 1.5255, 0.5048, 0.2599],
        [1.4374, 0.8905, 1.3153,  ..., 2.0713, 1.6836, 0.5696],
        [1.0323, 0.5866, 0.8701,  ..., 1.3816, 0.9792, 0.2350],
        ...,
        [1.0827, 0.2540, 0.9205,  ..., 1.4209, 0.2116, 0.0851],
        [2.3440, 0.3290, 1.7657,  ..., 2.6452, 0.0094, 0.0726],
        [1.9021, 1.1575, 0.9504,  ..., 2.8932, 3.0049, 0.8788]],
       device='cuda:0'), tensor([[-3.3234e-01,  3.8320e-01, -4.5309e-01, -2.1828e-01, -9.8257e-02,
          5.1297e-01,  2.6782e-01, -4.6474e-01,  1.4257e-01,  2.5180e-02],
        [-4.9437e-01,  1.4408e+00, -1.3277e+00, -6.7099e-01, -9.7303e-01,
          8.8376e-01,  2.0869e+00, -1.2241e+00, -1.1808e-02, -2.8721e-01],
        [-4.6851e-01,  7.0442e-01, -6.9035e-01, -3.0122e-01, -5.2087e-01,
          5.3680e-01,  1.0902e+00, -6.8811e-01, -5.8538e-03, -1.3450e-01],
        [-3.8456e-01,  1.2067e+00, -1.1718e+00, -5.1045e-01, -8.2506e-01,
          6.5987e-01,  1.7947e+00, -1.1293e+00, -4.2568e-02, -3.4021e-01],
        [-2.7274e-01,  3.9018e-01, -3.3987e-01, -2.0997e-01, -4.7986e-02,
          3.8628e-01,  1.9459e-01, -4.9655e-01,  8.9817e-02, -5.3336e-03],
        [-5.3955e-01,  9.6930e-01, -4.3728e-01, -5.3740e-01, -3.1237e-01,
          9.0856e-01,  5.2432e-01, -9.7843e-01, -1.6589e-02,  1.1809e-01],
        [-6.1927e-01,  9.8971e-01, -4.6812e-01, -5.3994e-01, -1.1190e-01,
          1.1268e+00,  4.0038e-01, -8.8170e-01,  1.3236e-01,  2.8846e-01],
        [-6.1496e-01,  8.5433e-01, -3.7244e-01, -4.0528e-01, -1.2288e-01,
          9.8819e-01,  5.1180e-01, -1.0179e+00, -6.4936e-02,  1.5224e-01],
        [-1.3656e-01,  1.3502e+00, -1.2716e+00, -5.1637e-01, -8.0389e-01,
          5.0476e-01,  1.7204e+00, -1.2148e+00,  6.4660e-02, -4.3935e-01],
        [-4.5191e-01,  6.9793e-01, -6.9946e-01, -2.1295e-01, -4.5126e-01,
          5.5428e-01,  1.0593e+00, -6.2475e-01,  2.7435e-02, -1.4229e-01],
        [-6.5243e-01,  5.1605e-01, -7.5748e-01, -3.2584e-01, -2.7858e-01,
          4.0852e-01,  1.1638e+00, -6.5702e-01, -1.0563e-01, -4.0021e-01],
        [-3.3032e-01,  7.2498e-01, -5.8736e-01, -2.6019e-01, -4.5436e-01,
          5.4228e-01,  8.2845e-01, -5.7566e-01,  1.7750e-01,  2.0357e-02],
        [-4.4480e-01,  7.3832e-01, -3.0329e-01, -2.9365e-01, -6.5846e-02,
          6.2764e-01,  2.6826e-01, -6.6427e-01,  8.6279e-02,  9.3627e-02],
        [-6.0604e-01,  8.4032e-01, -2.9799e-01, -2.8645e-01, -1.2404e-01,
          7.6286e-01,  4.5047e-01, -8.1904e-01,  6.9177e-02,  1.7305e-01],
        [-3.2032e-01,  2.8899e-01, -2.7930e-01, -1.0820e-01, -8.7813e-02,
          4.2989e-01,  2.3074e-01, -3.4492e-01,  1.8050e-01,  1.2800e-01],
        [-3.0513e-01,  3.9914e-01, -6.0646e-01, -2.6351e-01, -2.7356e-01,
          2.8505e-01,  5.8131e-01, -5.7404e-01,  1.5823e-01, -1.6866e-01],
        [-2.7636e-01,  1.1767e+00, -7.7438e-01, -3.8706e-01, -3.9303e-01,
          4.6782e-01,  1.0622e+00, -1.0762e+00,  4.9148e-02, -3.8802e-01],
        [-2.0828e-01,  6.2272e-01, -5.3900e-01, -3.3645e-01, -3.3247e-01,
          3.3811e-01,  5.9149e-01, -7.5154e-01,  1.0542e-01, -1.6061e-01],
        [-5.6043e-01,  1.5546e+00, -1.1319e+00, -6.3217e-01, -9.5229e-01,
          1.0010e+00,  1.9442e+00, -1.2402e+00,  6.3746e-02, -1.1447e-01],
        [-3.6728e-01,  8.4020e-01, -6.0588e-01, -6.0948e-01, -3.3202e-01,
          7.9167e-01,  5.0514e-01, -9.4783e-01,  7.6466e-02,  4.0921e-02],
        [-4.0687e-01,  5.9265e-01, -5.7716e-01, -3.9373e-01, -3.9421e-01,
          6.5722e-01,  5.4073e-01, -7.0664e-01,  1.6489e-01,  7.2794e-02],
        [-3.8380e-01,  2.7162e+00, -2.5627e+00, -1.1164e+00, -1.5925e+00,
          9.3481e-01,  3.7364e+00, -2.4238e+00, -1.0314e-01, -1.0064e+00],
        [-4.7202e-01,  3.5737e-01, -2.9735e-01,  4.0058e-02,  4.6591e-02,
          6.2818e-01,  3.5133e-01, -5.0691e-01,  8.5058e-02,  1.2860e-01],
        [-5.4331e-01,  8.7339e-01, -4.4107e-01, -5.6390e-01, -2.1569e-01,
          9.0422e-01,  4.4035e-01, -9.3362e-01,  3.3818e-02,  7.6983e-02],
        [-2.8284e-01,  8.6091e-01, -9.5771e-01, -6.9753e-01, -5.9229e-01,
          7.1449e-01,  8.6383e-01, -9.5688e-01,  2.8564e-01, -8.6956e-02],
        [-2.6472e-01,  3.2996e-01, -3.8305e-01, -3.0279e-01, -1.4940e-01,
          4.9750e-01,  1.9953e-01, -5.6123e-01,  1.4944e-01,  7.7885e-02],
        [-4.9790e-01,  7.2893e-01, -2.1630e-01, -3.1124e-01, -4.2198e-02,
          6.9824e-01,  4.0415e-01, -8.2430e-01, -1.0067e-02,  7.5870e-02],
        [-3.9930e-01,  7.5478e-01, -7.3627e-01, -4.2999e-01, -5.3460e-01,
          6.2404e-01,  9.1600e-01, -7.2025e-01,  1.6040e-01, -4.5667e-02],
        [-2.7529e-01,  5.2298e-01, -4.7631e-01, -2.5663e-01, -3.3334e-01,
          3.9231e-01,  5.0426e-01, -5.7331e-01,  1.5787e-01, -1.9326e-02],
        [-6.7750e-01,  1.2113e+00, -6.6469e-01, -8.7860e-01, -3.8889e-01,
          1.5783e+00,  6.3819e-01, -1.3879e+00, -3.4394e-03,  2.9871e-01],
        [-8.4327e-01,  1.1513e+00, -2.3281e-01, -3.9809e-01, -8.2300e-02,
          1.3202e+00,  7.8824e-01, -1.2817e+00, -5.8502e-02,  3.6043e-01],
        [-2.2427e-01,  7.6033e-01, -5.3150e-01, -3.8596e-01, -4.6665e-01,
          4.6180e-01,  5.4415e-01, -6.8848e-01,  1.9868e-01, -4.8581e-02],
        [-1.4217e-01,  4.5138e-01, -4.0347e-01, -1.6116e-01, -2.3004e-01,
          1.7173e-01,  4.6380e-01, -4.9218e-01,  9.3206e-02, -1.5342e-01],
        [-3.0451e-01,  6.8649e-01, -4.2646e-01, -4.0716e-01, -1.3898e-01,
          5.4627e-01,  4.2714e-01, -8.2387e-01,  7.9394e-02, -3.2368e-02],
        [-5.6144e-01,  6.8948e-01, -2.2214e-01, -1.8483e-01, -2.7363e-01,
          6.0474e-01,  5.8905e-01, -6.6441e-01,  7.1013e-02,  1.6045e-01],
        [-5.1365e-01,  7.8768e-01, -2.2792e-01, -1.9979e-01, -2.7445e-02,
          5.6401e-01,  3.5237e-01, -7.4596e-01,  4.3419e-02,  6.0165e-02],
        [-2.8649e-01,  6.6652e-01, -6.4209e-01, -5.9217e-01, -4.4069e-01,
          7.0135e-01,  4.7496e-01, -7.7144e-01,  1.6233e-01,  4.5916e-02],
        [-3.9754e-01,  1.2590e+00, -1.3090e+00, -4.5754e-01, -8.6424e-01,
          6.4841e-01,  1.9391e+00, -1.1355e+00, -2.9817e-02, -4.1126e-01],
        [-2.8088e-01,  9.2664e-01, -7.0853e-01, -2.7355e-01, -5.0471e-01,
          5.2748e-01,  1.0186e+00, -7.4919e-01,  1.3229e-01, -1.4096e-01],
        [-2.0940e-01,  7.5057e-01, -5.7186e-01, -2.5274e-01, -4.1091e-01,
          2.4368e-01,  7.8928e-01, -7.7606e-01,  4.0285e-02, -2.8006e-01],
        [-3.9296e-01,  5.5379e-01, -6.8573e-01, -3.2503e-01, -3.8608e-01,
          4.3842e-01,  8.6440e-01, -7.3545e-01,  7.3720e-02, -1.8310e-01],
        [-9.3550e-01,  1.2991e+00, -3.1317e-01, -4.8580e-01, -5.4436e-02,
          1.4131e+00,  8.5310e-01, -1.3915e+00, -1.0217e-01,  2.7140e-01],
        [-5.6825e-01,  7.4379e-01, -1.4981e-01, -1.8457e-01, -1.2473e-01,
          6.0977e-01,  4.3823e-01, -7.1198e-01,  6.0707e-02,  1.7459e-01],
        [-4.6342e-01,  7.6163e-01, -2.5227e-01, -2.4550e-01,  1.7283e-02,
          6.7014e-01,  4.6074e-01, -8.3850e-01, -1.6165e-02,  7.8360e-02],
        [-1.3088e+00,  5.5195e-01, -1.2635e+00, -5.2009e-01, -4.5959e-01,
          1.2487e+00,  1.5444e+00, -1.0715e+00, -7.5137e-02, -1.6492e-01],
        [-4.3710e-01,  7.6245e-01, -8.5870e-01, -3.2535e-01, -5.4223e-01,
          4.2909e-01,  1.2659e+00, -7.9370e-01, -4.2261e-02, -2.9572e-01],
        [-3.9228e-01,  8.3746e-01, -4.6793e-01, -4.3109e-01, -3.3747e-01,
          6.8960e-01,  5.3416e-01, -7.3318e-01,  1.7891e-01,  1.2127e-01],
        [-2.9100e-01,  1.4662e-01, -2.0870e-01, -2.2459e-03, -1.0719e-02,
          2.6572e-01,  1.4159e-01, -2.6277e-01,  1.3748e-01,  7.5757e-02],
        [-3.6244e-01,  5.9010e-01, -6.3286e-01, -6.5689e-01, -3.0122e-01,
          9.2553e-01,  3.5400e-01, -9.0272e-01,  4.9402e-02,  8.7073e-02],
        [-5.2329e-01,  7.9488e-01, -5.5508e-01, -6.3189e-01, -2.2209e-01,
          1.2244e+00,  4.8321e-01, -1.0993e+00,  1.1845e-02,  2.1963e-01],
        [-6.3390e-01,  6.8503e-01, -4.7962e-01, -2.5385e-01, -3.4360e-01,
          6.5701e-01,  8.3815e-01, -7.5416e-01,  5.5292e-02,  4.2524e-02],
        [-3.6309e-01,  3.3860e-01, -2.8214e-01, -2.4455e-01, -9.2532e-02,
          5.6071e-01,  3.1482e-01, -5.7395e-01,  1.0797e-01,  1.1878e-01],
        [-5.4590e-01,  7.4357e-01, -2.8198e-01, -3.1900e-01,  2.2036e-02,
          7.9276e-01,  3.5815e-01, -7.9517e-01,  2.4701e-02,  1.6994e-01],
        [-5.1801e-01,  7.8715e-01, -3.2375e-01, -3.9471e-01, -2.0687e-01,
          8.2500e-01,  4.3833e-01, -7.8184e-01,  8.7637e-02,  2.0082e-01],
        [-5.6319e-01,  8.5566e-01, -8.8812e-01, -3.3724e-01, -5.4378e-01,
          6.7215e-01,  1.4430e+00, -8.0539e-01, -6.3501e-02, -1.9973e-01],
        [-2.5892e-01,  6.0247e-01, -5.7024e-01, -2.4114e-01, -3.9344e-01,
          3.8097e-01,  7.8897e-01, -5.6944e-01,  6.1935e-02, -8.2775e-02],
        [-2.1175e-01,  6.6697e-01, -5.0294e-01, -2.1404e-01, -2.1800e-01,
          2.2777e-01,  5.2352e-01, -6.8486e-01,  1.2321e-01, -2.2914e-01],
        [-7.6895e-01,  1.0058e+00, -1.8083e-01, -3.0837e-01, -6.0512e-02,
          9.5108e-01,  5.9504e-01, -9.9211e-01,  2.8044e-02,  2.6991e-01],
        [-2.2851e-01,  7.8281e-01, -7.9325e-01, -2.8091e-01, -4.8346e-01,
          3.7708e-01,  1.0958e+00, -6.8743e-01,  3.5208e-02, -2.5835e-01],
        [-1.6308e-01,  4.6100e-01, -4.5950e-01, -1.9182e-01, -1.9397e-01,
          1.8745e-01,  4.5321e-01, -5.3567e-01,  9.7656e-02, -1.7842e-01],
        [-2.9410e-01,  8.8034e-01, -8.3378e-01, -6.8568e-01, -3.4597e-01,
          5.0111e-01,  8.1637e-01, -1.0676e+00,  2.2729e-01, -3.1829e-01],
        [-4.7486e-01,  9.7049e-01, -4.5957e-01, -5.6483e-01, -1.5181e-01,
          9.4012e-01,  4.9515e-01, -9.7031e-01,  5.5656e-02,  1.2873e-01],
        [-8.0780e-01,  1.0965e+00, -3.2220e-01, -3.7490e-01,  2.4659e-03,
          1.0762e+00,  6.0587e-01, -1.1244e+00, -6.1351e-02,  1.8774e-01],
        [-5.0702e-01,  1.1280e+00, -8.2931e-01, -5.5427e-01, -6.0676e-01,
          8.7622e-01,  1.0834e+00, -9.6462e-01,  2.1953e-01, -8.1100e-03],
        [-3.0166e-01,  3.8377e-01, -2.4739e-01, -2.0384e-01, -8.8669e-02,
          4.7209e-01,  1.8716e-01, -5.1473e-01,  5.0085e-02,  1.0020e-01],
        [-5.3537e-01,  6.9469e-01, -2.7861e-01, -2.2268e-01,  8.5776e-03,
          7.2741e-01,  3.5829e-01, -7.6389e-01, -4.7722e-02,  8.2707e-02],
        [-3.0907e-01,  5.3749e-01, -4.5499e-01, -3.2610e-01, -3.1695e-01,
          4.8024e-01,  4.9396e-01, -6.6118e-01,  1.2770e-01,  1.5544e-02],
        [-1.3540e+00,  9.4823e-01, -1.4290e+00, -7.4309e-01, -5.8443e-01,
          9.1833e-01,  2.4040e+00, -1.2632e+00, -2.5036e-01, -7.4381e-01],
        [-5.5004e-01,  7.5205e-01, -1.8466e-01, -2.7518e-01, -7.6564e-02,
          7.7863e-01,  4.1801e-01, -7.3260e-01,  1.0257e-01,  2.6784e-01],
        [-5.0120e-01,  9.1259e-01, -5.9397e-01, -4.1461e-01, -4.3003e-01,
          7.6656e-01,  9.2179e-01, -8.1496e-01,  1.5497e-01,  9.5488e-02],
        [-1.6077e-01,  1.2918e+00, -1.1651e+00, -4.6528e-01, -7.5177e-01,
          4.6617e-01,  1.5873e+00, -1.0513e+00,  1.1659e-01, -3.8483e-01],
        [-5.2688e-01,  7.4076e-01, -3.0049e-01, -3.2582e-01, -7.8447e-02,
          7.8883e-01,  4.0903e-01, -8.0828e-01, -3.3750e-02,  1.1121e-01],
        [-2.3935e-01,  1.1577e+00, -1.1437e+00, -3.9295e-01, -7.3881e-01,
          5.5257e-01,  1.6290e+00, -9.6706e-01,  1.5561e-02, -3.5753e-01],
        [-4.1900e-01,  3.7637e-01, -4.4672e-01, -1.8582e-01, -2.8258e-01,
          3.4193e-01,  6.5232e-01, -5.0776e-01,  1.3179e-02, -1.1264e-01],
        [-5.4361e-01,  1.2793e+00, -9.4385e-01, -6.5908e-01, -7.5179e-01,
          9.4817e-01,  1.4359e+00, -1.1261e+00,  1.5195e-01, -1.9385e-02],
        [-2.9923e-01,  3.7956e-01, -6.4492e-01, -5.4944e-01, -2.2099e-01,
          6.8399e-01,  3.5969e-01, -7.6037e-01,  2.0250e-01,  4.4216e-02],
        [-2.9004e-01,  4.5861e-01, -5.5469e-01, -1.8433e-01, -2.3146e-01,
          3.5770e-01,  6.4199e-01, -4.6606e-01,  8.7959e-02, -1.3841e-01],
        [-3.0589e-01,  5.8956e-01, -2.7834e-01, -1.8313e-01, -2.4610e-01,
          3.9961e-01,  4.0977e-01, -5.8302e-01,  1.1561e-01,  3.4700e-02],
        [-6.3618e-01,  8.2308e-01, -2.3083e-01, -3.5111e-01,  3.0333e-04,
          9.7969e-01,  4.8694e-01, -8.8910e-01,  8.7584e-02,  3.2619e-01],
        [-4.9841e-01,  2.2495e+00, -2.2439e+00, -9.6469e-01, -1.5332e+00,
          9.6959e-01,  3.3755e+00, -1.9712e+00, -1.6561e-01, -7.4703e-01]],
       device='cuda:0'))